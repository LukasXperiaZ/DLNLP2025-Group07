{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_and_clean_wikipedia(xml_bz2_file, output_file):\n",
    "    \"\"\"\n",
    "    Extracts text content from English Wikipedia XML dump, performs basic cleaning,\n",
    "    and writes each sequence to a new line in the output file.\n",
    "    \"\"\"\n",
    "    with bz2.open(xml_bz2_file, 'rb') as bz2file, \\\n",
    "            open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        context = ET.iterparse(bz2file, events=('start', 'end'))\n",
    "        for event, elem in context:\n",
    "            if event == 'end' and elem.tag.endswith('text'):\n",
    "                text = elem.text\n",
    "                if text:\n",
    "                    # Basic cleaning: remove newlines and extra whitespace\n",
    "                    cleaned_text = text.strip().replace('\\n', ' ')\n",
    "                    outfile.write(cleaned_text + '\\n')\n",
    "                # Clear the element from memory to handle large files\n",
    "                elem.clear()\n",
    "    print(f\"Extracted and cleaned Wikipedia data is now in: {output_file}\")\n",
    "\n",
    "# Specify the path to your downloaded Wikipedia XML.bz2 file\n",
    "wikipedia_file = 'enwiki-latest-pages-articles-multistream.xml.bz2'\n",
    "# Specify the desired output file name\n",
    "output_file = 'dump.txt'\n",
    "\n",
    "extract_and_clean_wikipedia(wikipedia_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
