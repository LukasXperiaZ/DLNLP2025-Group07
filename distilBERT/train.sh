python train.py \
--student_type distilbert \
--student_config training_configs/distilbert-small-uncased.json \
--student_pretrained_weights serialization_dir/distilBERT_small_4_layer/model_epoch_0.pth \
--teacher_type bert \
--teacher_name bert-base-uncased \
--alpha_ce 5.0 --alpha_mlm 1.0 --alpha_cos 3.0 --alpha_clm 0.0 --mlm \
--batch_size 32 \
--n_epoch 1 \
--fp16 \
--freeze_pos_embs \
--dump_path serialization_dir/distilBERT_small_4_layer_2_epochs \
--data_file data/binarized_text.bert-base-uncased.pickle \
--token_counts data/token_counts.bert-base-uncased.pickle \
--force