{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ac994b12-69e0-49ae-b127-a687495adf84",
   "metadata": {},
   "source": [
    "author: Christian Swoboda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecafd4-1368-4d1c-8cb3-247291041ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets   # if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8150eaf4-2b4c-47ae-a367-c2d3315336a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, DistilBertConfig, DistilBertForSequenceClassification)\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# =====================\n",
    "# DEVICE CONFIGURATION\n",
    "# =====================\n",
    "# Use \"mps\" on macOS or \"cuda\" if available or \"cpu\" as fallback\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "10bf0e1f-e8cc-4526-b57e-46839b2f2c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preprocessing Data complete!\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# LOAD AND PREPROCESS IMDB DATASET\n",
    "# =================================\n",
    "# Load the IMDb dataset (50.000 labeled reviews - 25.000 train / 25.000 test)\n",
    "subset_size = 25000\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(subset_size))  # use subset_size random samples fro training\n",
    "test_data = dataset[\"test\"].shuffle(seed=42).select(range(subset_size))    # use subset_size random samples for testing\n",
    "\n",
    "# Load BERT tokenizer (bert-base-uncased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function (max length 128)\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Apply tokenizer to training data and testing data\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "test_data = test_data.map(tokenize, batched=True)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8)\n",
    "\n",
    "print(\"\\nLoading and preprocessing Data complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "83b946d1-1a13-4870-9430-8dbfb37acf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"imdb\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "87d606da-617d-42e0-87e9-ed9797842b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"This is a bad movie. Not one of the funny bad ones either. This is a lousy bad one. It was actually painful to watch. The direction was awful,with lots of jumping around and the green and yellow hues used throughout the movie makes the characters look sickly. Keira Knightly was not convincing as a tough chick at all,and I cannot believe Lucy Liu and Mickey Rourke signed on for this criminal waste of celluloid. The script was terrible and the acting was like fingernails across a chalkboard. If you haven't seen it,don't. You are not missing anything and will only waste two hours of your life watching this drivel .I have seen bad movies before and even enjoyed them due to their faults. This one is just a waste of time.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e2a66a42-b8f6-4fd2-9c8a-6e41decb5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher model definition complete!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# DEFINE TEACHER MODEL (BERT)\n",
    "# ===========================\n",
    "# Load pretrained BERT model for sequence classification to device\n",
    "teacher = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "print(\"\\nTeacher model definition complete!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65598fdb-90df-43ce-a970-79d40b658a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started with 10 epochs ...\n",
      "[Epoch 1] Training Loss: 1036.8148  Epoch Time: 0:02:59.928827\n",
      "[Epoch 2] Training Loss: 555.1100  Epoch Time: 0:03:00.004059\n",
      "[Epoch 3] Training Loss: 284.8935  Epoch Time: 0:03:00.006631\n",
      "[Epoch 4] Training Loss: 183.4840  Epoch Time: 0:02:59.969044\n",
      "[Epoch 5] Training Loss: 132.1971  Epoch Time: 0:02:59.998647\n",
      "[Epoch 6] Training Loss: 107.6060  Epoch Time: 0:02:59.962089\n",
      "[Epoch 7] Training Loss: 99.4189  Epoch Time: 0:02:59.976020\n",
      "[Epoch 8] Training Loss: 84.9268  Epoch Time: 0:02:59.999456\n",
      "[Epoch 9] Training Loss: 70.9583  Epoch Time: 0:02:59.988552\n",
      "[Epoch 10] Training Loss: 67.9117  Epoch Time: 0:02:59.996357\n",
      "\n",
      "Training complete in 0:29:59.841565!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRAINING LOOP (BERT)\n",
    "# ===========================\n",
    "\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "epochs_count = 10\n",
    "print(f\"\\nTraining started with {epochs_count} epochs ...\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(teacher.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(epochs_count):  # train epochs_count epochs\n",
    "    teacher.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Start epoch timer\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move input data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Get teacher predictions\n",
    "        teacher_logits = teacher(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Compute loss with true labels\n",
    "        loss = nn.CrossEntropyLoss()(teacher_logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Stop the epoch timer\n",
    "    epoch_end_time = time.time()\n",
    "\n",
    "    epoch_time = timedelta(seconds=(epoch_end_time - epoch_start_time))\n",
    "    print(f\"[Epoch {epoch+1}] Training Loss: {total_loss:.4f}  Epoch Time: {epoch_time}\")\n",
    "\n",
    "# Stop training timer\n",
    "end_time = time.time()\n",
    "training_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "print(f\"\\nTraining complete in {training_time}!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8c222d19-9b2c-4f29-9193-ca3d4105e5bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation started ...\n",
      "BERT Teacher Accuracy: 0.8757\n",
      "\n",
      "Evaluation complete in 0:00:47.097315!\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# EVALUATE TEACHER MODEL \n",
    "# =======================\n",
    "\n",
    "# Start evaluation timer\n",
    "start_time = time.time()\n",
    "\n",
    "print (\"\\nEvaluation started ...\")\n",
    "\n",
    "teacher.eval()\n",
    "teacher_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = teacher(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        teacher_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "teacher_accuracy = accuracy_score(true_labels, teacher_preds)\n",
    "print(f\"BERT Teacher Accuracy: {teacher_accuracy:.4f}\")\n",
    "\n",
    "# Stop evaluation timer\n",
    "end_time = time.time()\n",
    "evaluation_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "print(f\"\\nEvaluation complete in {evaluation_time}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "41db15d4-7473-4dc9-9635-e2ee7d939c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student model definition complete!\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# DEFINE STUDENT MODEL (DistilBERT)\n",
    "# ==================================\n",
    "# Create student model with BERT's hidden size and intermediate layer size, but only 6 layers\n",
    "student_config = DistilBertConfig(\n",
    "    num_labels=2,\n",
    "    n_layers=6,           # DistilBERT with half the layers of BERT\n",
    "    dim=768,              # hidden size of BERT\n",
    "    hidden_dim=3072,      # intermediate (feed-forward) layer size of BERT\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1\n",
    ")\n",
    "student = DistilBertForSequenceClassification(student_config).to(device)\n",
    "\n",
    "print(\"\\nStudent model definition complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "43e0341c-c39d-4146-beba-376333d29198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# DISTILLATION LOSS FUNCTION\n",
    "# ===========================\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    student_logits: raw output from student model\n",
    "    teacher_logits: raw output from teacher model\n",
    "    labels: ground truth labels (0 ... negative, 1 ... positve)\n",
    "    T: temperature for softening the distribution\n",
    "    alpha: tradeoff between soft and hard loss\n",
    "    \"\"\"\n",
    "    # soft target loss: student follows the teacher's behaviour\n",
    "    # Kullback-Leibler divergence between softened distributions\n",
    "    # (shows how much the student deviates from the teacher)\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "        nn.functional.log_softmax(student_logits / T, dim=1),\n",
    "        nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    ) * (T * T)\n",
    "\n",
    "    # hard target loss: student predicts the true label\n",
    "    # standard classification loss with the true labels\n",
    "    hard_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "\n",
    "    # combining soft target loss and hard target loss\n",
    "    # alpha: how much loss from teacher (soft) and how much from true labels (hard)\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0179c785-c08e-4fde-94a0-f308a0558485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 10 epochs (alpha = 0.0 ; T = 4.0)\n",
      "[Epoch 1] Training Loss: 1534.3619  Epoch Time: 0:02:25.156244\n",
      "[Epoch 2] Training Loss: 989.2915  Epoch Time: 0:02:25.381793\n",
      "[Epoch 3] Training Loss: 778.0316  Epoch Time: 0:02:25.348408\n",
      "[Epoch 4] Training Loss: 625.4322  Epoch Time: 0:02:25.325389\n",
      "[Epoch 5] Training Loss: 490.6610  Epoch Time: 0:02:25.381561\n",
      "[Epoch 6] Training Loss: 382.2016  Epoch Time: 0:02:25.318443\n",
      "[Epoch 7] Training Loss: 280.5184  Epoch Time: 0:02:25.363282\n",
      "[Epoch 8] Training Loss: 224.5050  Epoch Time: 0:02:25.340927\n",
      "[Epoch 9] Training Loss: 166.3939  Epoch Time: 0:02:25.326895\n",
      "[Epoch 10] Training Loss: 135.3951  Epoch Time: 0:02:25.316045\n",
      "\n",
      "Training complete in 0:24:13.268500!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRAINING LOOP (DistilBERT)\n",
    "# ===========================\n",
    "\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "epochs_count = 10\n",
    "T = 4.0\n",
    "alpha = 0.0    # no teacher, only true labels\n",
    "#alpha = 0.25\n",
    "#alpha = 0.5    # half teacher, half true labels\n",
    "#alpha = 0.75\n",
    "#alpha = 1.0    # only teacher, no true labels\n",
    "\n",
    "print(f\"\\nTraining with {epochs_count} epochs (alpha = {alpha} ; T = {T})\")\n",
    "\n",
    "teacher.eval()  # \"freeze\" teacher weights\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(epochs_count):  # train epochs_count epochs\n",
    "    student.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Start epoch timer\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move input data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Get teacher predictions without computing gradients\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Get student predictions\n",
    "        student_logits = student(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Compute distillation loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels, T, alpha)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Stop the epoch timer\n",
    "    epoch_end_time = time.time()\n",
    "\n",
    "    epoch_time = timedelta(seconds=(epoch_end_time - epoch_start_time))\n",
    "    print(f\"[Epoch {epoch+1}] Training Loss: {total_loss:.4f}  Epoch Time: {epoch_time}\")\n",
    "\n",
    "# Stop training timer\n",
    "end_time = time.time()\n",
    "training_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "print(f\"\\nTraining complete in {training_time}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "331228f2-9e71-48b5-bda1-5e08c2c7b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation started ...\n",
      "\n",
      "DistilBERT Student Accuracy: 0.7603\n",
      "BERT Teacher Accuracy: 0.8757\n",
      "\n",
      "Evaluation complete in 0:00:25.506938!\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# EVALUATE STUDENT MODEL\n",
    "# =======================\n",
    "\n",
    "# Start evaluation timer\n",
    "start_time = time.time()\n",
    "\n",
    "print (\"\\nEvaluation started ...\")\n",
    "\n",
    "student.eval()\n",
    "student_preds, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        student_preds.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "student_accuracy = accuracy_score(true_labels, student_preds)\n",
    "print(f\"\\nDistilBERT Student Accuracy: {student_accuracy:.4f}\")\n",
    "print(f\"BERT Teacher Accuracy: {teacher_accuracy:.4f}\")\n",
    "\n",
    "# Stop evaluation timer\n",
    "end_time = time.time()\n",
    "evaluation_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "print(f\"\\nEvaluation complete in {evaluation_time}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa7b7a-4a4c-4ca3-bcaa-f77f4b6a452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
